### 决策树

仅仅是做简单的是否判断

使用数据来决定如何将房屋分为两组，然后再次确定每组的预测价格。从数据中捕获模式的这一步骤称为拟合或训练模型。用于拟合模型的数据称为训练数据

+ 优化决策时树

  目的是选出正确的决策树，选出数据更加合理的部分

决策树可以根据条件越来越深

## 基础数据探索

> 还好在这部分之前就学习了pandas，因此会显得比较简单，所以就不多讲了，具体的倒回去看pandas

## Building ur first model

这是个令人振奋的标题，这意味着我们正式接触到了机器学习

构建模型我们主要关注

+ 建模数据
+ 选择特征
+ 预测目标

### 建模数据

还是涉及到pandas的操作，主要关注变量的选择，以及怎么处理缺失值

```python
melbourne_data.columns //这样来查看变量
melbourne_data = melbourne_data.dropna(axis=0) //处理缺失值
# dropna drops missing values (think of na as "not available")
```

### 预测目标

预测price变量

```
 y = reviews.price
```

将其price单独作为一个series对待

### 选择特征

我们将构建模型的列称为特征

例如要选择多个特征

```python
model_feature = ['rooms','price','xxx','xxx']
X = reviews[model_feature]
```

### 创建模型

构建模型和使用模型的步骤

+ Define（定义，它是什么类型的模型，决策树？）
+ Fit （拟合，从数据中捕获模式）
+ Predict（预测）
+ Evaluate （评估）

在创建之前，我们最好先了解了解拟合的基础概念，不然会很困难

拟合：通过训练数据对模型的参数进行调整，使得模型能够尽可能准确地表示输入数据和预期输出之间的关系。

通俗来说就是将很多点进行连接，从而形成了实体的关系

看个简单的例子,了解大致的建模流程

```python
from sklearn.tree import DecisionTreeRegressor
import pandas as pd

home_data = pd.read_csv('melb_data.csv')
#数据的选择
model_feature = ['Rooms','Distance','Postcode','Bedroom2','Bathroom','Car','Landsize','Propertycount']
y = home_data.Price
X = home_data[model_feature]
#为 random_state 指定数字可确保您在每次运行中获得相同的结果
first_model = DecisionTreeRegressor(random_state=1)
#拟合
first_model.fit(X, y)
#使用模型进行预测

predictions = first_model.predict(X)
print(predictions)
print(y)
```

## 模型验证

测试模型的性能，验证其是否优良

很直观的一点，我们不能直接将训练数据用来对模型进行验证，我们需要先了解第一个概念平均绝对误差(**Mean Absolute Error**,MAE)

+ error=actual−predicted

  从误差开始，例如一栋房子的售价是15000，预测结果为10000，那么5000这个值就是误差

对于误差的处理，根据MAE标准，我们取每个误差的绝对值，并对所有的误差求平均

> On average, our predictions are off by about X.
> 平均而言，我们的预测偏差约为 X。

我们通过上一个模型来试着计算该模型的MAE

```python
predictions = first_model.predict(X)
print(mean_absolute_error(y, predictions))
```

![image-20240414235716014](https://raw.githubusercontent.com/uu2fu3o/blog-picture/master/cloud/image-20240414235716014.png)

误差好像有点太大了QAQ

### The Problem with "In-Sample" Scores

"样本内"分数问题，我们刚才计算的可以称作为样本内分数，这个分数很显然和我们的数据有关

kaggle上举了一个例子，房价的高低和门的颜色无关，但是由于提供的数据中绿色的门房价总是很高，因此该模式自然会认为绿色的门是高房价的一个条件。但事实上这是无关的。这导致了一个问题，当模式遇到新的数据时，可能无法处理这些数据，导致的偏差过大。

因此，最好的做法是排除无关的数据进行训练(可以理解为脏数据)

scikit-learn 库有一个函数 `train_test_split` 将数据分成两部分。我们将使用其中一些数据作为训练数据来拟合模型，并使用其他数据作为验证数据来计算 `mean_absolute_error`

```python
from sklearn.metrics import mean_absolute_error
from sklearn.tree import DecisionTreeRegressor
import pandas as pd
from sklearn.model_selection import train_test_split

home_data = pd.read_csv('melb_data.csv')
#数据的选择
model_feature = ['Rooms','Distance','Postcode','Bedroom2','Bathroom','Car','Landsize','Propertycount']
y = home_data.Price
X = home_data[model_feature]
train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)
#为 random_state 指定数字可确保您在每次运行中获得相同的结果
first_model = DecisionTreeRegressor(random_state=1)
#拟合
first_model.fit(train_X, train_y)
#使用模型进行预测

predictions = first_model.predict(val_X)
print(mean_absolute_error(val_y, predictions))
```

![image-20240415001257527](https://raw.githubusercontent.com/uu2fu3o/blog-picture/master/cloud/image-20240415001257527.png)

显然MAE非常大，我们需要优化模型来使MAE变小

## Underfitting and Overfitting 

### 过拟合

这是模型训练中一种常见的现象

对于我们当前使用的决策树模型来说，如果深度过多，那么每一个house的数据就会很少，虽然模式能对训练数据做出优良的预测，但是在面对新数据时就会变得不可靠

> 可以这样理解，模型在训练时，学习到了数据的“噪声”，而非真正的模式，而新数据并的噪声是未知的，无法预测的
>
> 这里的噪声是数据集中的那些不反映真实情况、无法代表基本模式或者趋势的信息，它们是数据中的随机误差或者无关变量，例如我们要判断一张图片中是否有火，显然图片中并不一定只有火，模型可能学习到了其他的内容例如树之类的
>
> 对于纯数据来说，可能是测量误差，采样误差，非相关他也整，记录错误之类的

### 欠拟合

对于我们的决策树模型来说，如果我们深度过浅，我们可能只有2-4个houses,这样的话每个组中将有大量的数据，这导致了模型甚至无法捕获训练数据的模式和区别，更不用说新数据了

### Example

同样是决策树，我们可以主动去控制深度和叶子，从而降低MAE

![image-20240415162231197](https://raw.githubusercontent.com/uu2fu3o/blog-picture/master/cloud/image-20240415162231197.png)

我们始终希望欠拟合和过拟合达到一个平衡

## Random Forests

我们试着使用更加附加的算法，随机森林，随机森林中有许多树。比单个树好很多

```python
import pandas as pd
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

home_data = pd.read_csv('melb_data.csv')
model_feature = ['Rooms','Distance','Bedroom2','Bathroom','Landsize','Propertycount']
y = home_data.Price
X = home_data[model_feature]
train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)
model = RandomForestRegressor(random_state=1)
model.fit(train_X,train_y)
predictions = model.predict(val_X)
print(mean_absolute_error(val_y, predictions))
```

![image-20240415170048972](https://raw.githubusercontent.com/uu2fu3o/blog-picture/master/cloud/image-20240415170048972.png)

即使我们采取默认值，也要比筛选出的决策树平衡点好很多
