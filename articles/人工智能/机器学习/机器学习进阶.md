## Missing Values

学习该如何处理数据集中的缺失值，为了改进模型

### 直接删除有缺失值的列

显然这是最快最有效的办法，但同样也是非常极端的方法，如果涉及到重要部分的内容，不建议这样做

### Imputation

数据插补是更好的办法，至少我们不会丢失数据了

例如，我们可以在缺失值的位置填上每列的平均值，这是一个估算的值

```python
from sklearn.impute import SimpleImputer
# Imputation
my_imputer = SimpleImputer()
imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))
imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid)) //不使用fit_transform方法，保证数据的统一性

# Imputation removed column names; put them back
imputed_X_train.columns = X_train.columns
imputed_X_valid.columns = X_valid.columns
```

### 插补扩展

我们可能遇到没法求平均值的情况，同样会有唯一值的情况出现例如(true, false)。这种情况下，我们可以建立标识，指出哪一条数据是最初缺失的，引用kaggle的图片

![image-20240415204110008](https://raw.githubusercontent.com/uu2fu3o/blog-picture/master/cloud/image-20240415204110008.png)

关于这样做是否有用，需要根据具体情况

## 分类变量

> what is **categorical variable**，
>
> 例如有1个特征为汽车的品牌，我们会有一些选项可以选择，像宝马，本田等等，这种变量就称为分类变量
>
> 数据中如果存在某些特殊字符，显然是会报错的，所以对于分类变量，我们需要先进行预处理再用于训练

```python
s = (X_train.dtypes == 'object')
object_cols = list(s[s].index)

print("Categorical variables:")
print(object_cols)
```

找出分类变量

### 删除分类变量

最直接的方法仍然是删除，这样做对于大量数据来说，显然有坏处

```python
X_train.select_dtypes(exclude=['object'])  //使用select_dtypes()方法，exclude排除指定类型
```

### 序数编码

对于不同的类别，进行不同的编码

对于例子吃早饭的频率来说，我们可以这样编码

```
every day   3
never  0
rarely 1
most days 2 
never  0
```

这样按照大小顺序编码是正确的，也符合我们吃早饭的频率排序

注意：将序数编码器拟合到训练数据中的列会为训练数据中出现的每个唯一值创建相应的整数值标签。如果验证数据包含的值未出现在训练数据中，则编码器将抛出错误，因为这些值不会分配有整数。

一般这种情况我们选择将”不好“的列进行删除

```python
# Categorical columns in the training data
object_cols = [col for col in X_train.columns if X_train[col].dtype == "object"]

# Columns that can be safely ordinal encoded
good_label_cols = [col for col in object_cols if 
                   set(X_valid[col]).issubset(set(X_train[col]))]
        
# Problematic columns that will be dropped from the dataset
bad_label_cols = list(set(object_cols)-set(good_label_cols))

from sklearn.preprocessing import OrdinalEncoder
# Drop categorical columns that will not be encoded
label_X_train = X_train.drop(bad_label_cols, axis=1)
label_X_valid = X_valid.drop(bad_label_cols, axis=1)

# Apply ordinal encoder 
ordinal_encoder = OrdinalEncoder()
label_X_train[good_label_cols] = ordinal_encoder.fit_transform(X_train[good_label_cols])
label_X_valid[good_label_cols] = ordinal_encoder.transform(X_valid[good_label_cols])
```

### One-Hot Encoding

One-hot 编码创建新列，指示原始数据中每个可能值的存在（或不存在）

这适用于我们的变量中没有太多的分类，超过15会表现得较差

![image-20240416234650548](https://raw.githubusercontent.com/uu2fu3o/blog-picture/master/cloud/image-20240416234650548.png)

类似于一个原始列的扩展

```python
from sklearn.preprocessing import OneHotEncoder

# Use as many lines of code as you need!
OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))
OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))
# One-hot encoding removed index; put it back
OH_cols_train.index = X_train.index
OH_cols_valid.index = X_valid.index
num_X_train = X_train.drop(object_cols, axis=1)
num_X_valid = X_valid.drop(object_cols, axis=1)
# Add one-hot encoded columns to numerical features
OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)
OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)
# Ensure all columns have string type
OH_X_train.columns = OH_X_train.columns.astype(str)
OH_X_valid.columns = OH_X_valid.columns.astype(str)
# Check your answer
step_4.check()
```

- 设置 `handle_unknown='ignore'` 以避免验证数据包含训练数据中未表示的类时出现错误
- 设置 `sparse=False` 确保编码列作为 numpy 数组（而不是稀疏矩阵）返回。

## Piplines

pipline用于将预处理数据和建模捆绑起来，这样的好处是让代码看起来更简单，更清晰等等

使用 ColumnTransformer 类将不同的预处理步骤捆绑在一起

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

home_data = pd.read_csv('melb_data.csv')
y = home_data.Price
X = home_data.drop(['Price'], axis=1)

X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=0)
#筛选出其中的分类变量以及浮点数等
categorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and X_train_full[cname].dtype == "object"]
numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]
#定义我们的训练特征
my_cols = categorical_cols + numerical_cols
X_train = X_train_full[my_cols].copy()
X_valid = X_valid_full[my_cols].copy()
#首先定义处理浮点数缺失值的部分,我们采用插补,全部换为0
numerical_transformer = SimpleImputer(strategy='constant')
#接着定义分类变量,但是我们需要同时处理missing value一个计算one hot,此时就可以采用piplines
categorical_transformer = Pipeline(steps=[
    ('imputer',SimpleImputer(strategy='most_frequent')),
    ('one-hot',OneHotEncoder(handle_unknown='ignore'))
])
#使用 ColumnTransformer 类将不同的预处理步骤捆绑在一起
processer = ColumnTransformer(transformers=[
    ('num',numerical_transformer, numerical_cols),
    ('cate',categorical_transformer,categorical_cols)
])
#定义模型
model = RandomForestRegressor(n_estimators=100, random_state=0)

#将预处理数据和建模绑定在一起
final_model = Pipeline(steps=[
    ('prcesser',processer),
    ('model',model)
])
#拟合
final_model.fit(X_train,y_train)
#预测
predictions = final_model.predict(X_valid)
#计算MAE评估
print(mean_absolute_error(y_valid,predictions))
```

## Cross-Validation

为什么需要交叉验证？

如果我们将数据集划分为训练和验证，用于训练的数据会减少，模型学习到噪声的概率会变大。

另外一种可能是由于我们将0.2的数据用于验证，5个0.2。我们无法知道模型在哪个0.2上表现更好。

### 什么是交叉验证

这部分只需要看图就很好理解

![image-20240420145840422](https://raw.githubusercontent.com/uu2fu3o/blog-picture/master/cloud/image-20240420145840422.png)

(图片来源于kaggle)

显然，这里每20%为一组，统共进行了5次实验，每次将不同的组作为验证集，其他的组作为训练集

每一次实验hold o8ut set会被用于模型质量的估计

在某个时候，100%的数据被holdout,我们得到了一个基于数据集中所有行的模型质量度量

### When we use

首先，使用交叉验证进行训练，需要消耗较多的时间，我们必须衡量这是否值得

因此，我们通常在数据集较小的时候使用

```python
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt


home_data = pd.read_csv('melb_data.csv')
y = home_data.Price
X = home_data.drop(['Price'], axis=1)

# X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=0)
#筛选出其中的分类变量以及浮点数等
categorical_cols = [cname for cname in X.columns if X[cname].nunique() < 10 and X[cname].dtype == "object"]
numerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]
#定义我们的训练特征
my_cols = categorical_cols + numerical_cols
X_train = X[my_cols].copy()
#首先定义处理浮点数缺失值的部分,我们采用插补,全部换为0
numerical_transformer = SimpleImputer(strategy='constant')
#接着定义分类变量,但是我们需要同时处理missing value一个计算one hot,此时就可以采用piplines
categorical_transformer = Pipeline(steps=[
    ('imputer',SimpleImputer(strategy='most_frequent')),
    ('one-hot',OneHotEncoder(handle_unknown='ignore'))
])
#使用 ColumnTransformer 类将不同的预处理步骤捆绑在一
processer = ColumnTransformer(transformers=[
    ('num',numerical_transformer, numerical_cols),
    ('cate',categorical_transformer,categorical_cols)
])
results ={}
def get_score(n_estimators):
    final_model = Pipeline(steps=[
        ('prcesser', processer),
        ('model', RandomForestRegressor(n_estimators=n_estimators,random_state=0))
    ])
    scores = -1 * cross_val_score(final_model, X_train, y, cv=5,
                              scoring='neg_mean_absolute_error')
    return scores.mean()

for i in range(1,9):
    results[i] = get_score(i*50)
    print(results[i])
```

我们不再需要自己划分训练集和验证集，只需要使用cross_val_score函数即可

![image-20240420155731382](https://raw.githubusercontent.com/uu2fu3o/blog-picture/master/cloud/image-20240420155731382.png)

质量比之前提升不少，并且数量最优为200

### 其他交叉验证

事实上，我们这里所讲的交叉验证只是其中一种非穷举的交叉验证k-flod，交叉验证可以细分为很多种类

[机器之心|交叉验证](https://www.jiqizhixin.com/graph/technologies/de51a6aa-a6c6-43b1-880f-79c0b033bbae)

## XGBoost

> **ensemble methods** combine the predictions of several models 
>
> 随机树森林在这里被称为集合方法，因为它收集了多个决策树的预测
>
> 另一种集成方法，则在这里称为梯度提升

### Gradient Boosting

梯度提升是一种通过循环将模型迭代添加到集合中的方法。

![image-20240421141506856](https://raw.githubusercontent.com/uu2fu3o/blog-picture/master/cloud/image-20240421141506856.png)

+ 初始模型做出预测
+ 预测的结果用于计算损失函数(eg:mse)
+ 使用损失函数拟合一个新的模型，将其添加到集合当中。具体来说，确定一个新的模型参数，将新模型添加到集合当中以减少损失(这里的梯度指，在损失函数的基础上使用梯度下降算法确定的新模型的参数)
+ 将新模型添加到集合中，重复

关于梯度下降算法，推荐看[梯度下降算法详解](https://www.cnblogs.com/pinard/p/5970503.html)，简单理解就好

```python
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from xgboost import XGBRegressor
import pandas as pd
from sklearn.metrics import mean_absolute_error

home_data = pd.read_csv('melb_data.csv')
y = home_data.Price
X = home_data.drop(['Price'], axis=1)

X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=0)
#筛选出其中的分类变量以及浮点数等
categorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and X_train_full[cname].dtype == "object"]
numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]
#定义我们的训练特征
my_cols = categorical_cols + numerical_cols
X_train = X_train_full[my_cols].copy()
X_valid = X_valid_full[my_cols].copy()
numerical_transformer = SimpleImputer(strategy='constant')
#接着定义分类变量,但是我们需要同时处理missing value一个计算one hot,此时就可以采用piplines
categorical_transformer = Pipeline(steps=[
    ('imputer',SimpleImputer(strategy='most_frequent')),
    ('one-hot',OneHotEncoder(handle_unknown='ignore'))
])
#使用 ColumnTransformer 类将不同的预处理步骤捆绑在一
processer = ColumnTransformer(transformers=[
    ('num',numerical_transformer, numerical_cols),
    ('cate',categorical_transformer,categorical_cols)
])
#使用梯度提升
model = XGBRegressor(random_state=0,n_estimators=200, learning_rate=0.05, n_jobs=4)

final_model = Pipeline(steps=[
    ('processer',  processer),
    ('model',model)
])

final_model.fit(X_train,y_train)

prediction= final_model.predict(X_valid)
print(mean_absolute_error(y_valid,prediction))#158936.2793917986
```

对比上一种集成方法，有了明显的提升

**Pipline是不允许我们设置early_stopping_rounds和eval_set参数的**，因此

```python
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from xgboost import XGBRegressor
import pandas as pd
from sklearn.metrics import mean_absolute_error

home_data = pd.read_csv('melb_data.csv')
y = home_data.Price
X = home_data.drop(['Price'], axis=1)

X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=0)
#筛选出其中的分类变量以及浮点数等
categorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and X_train_full[cname].dtype == "object"]
numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]
#定义我们的训练特征
my_cols = categorical_cols + numerical_cols
X_train = X_train_full[my_cols].copy()
X_valid = X_valid_full[my_cols].copy()
numerical_transformer = SimpleImputer(strategy='constant')
#接着定义分类变量,但是我们需要同时处理missing value一个计算one hot,此时就可以采用piplines
categorical_transformer = Pipeline(steps=[
    ('imputer',SimpleImputer(strategy='most_frequent')),
    ('one-hot',OneHotEncoder(handle_unknown='ignore'))
])
#使用 ColumnTransformer 类将不同的预处理步骤捆绑在一
processer = ColumnTransformer(transformers=[
    ('num',numerical_transformer, numerical_cols),
    ('cate',categorical_transformer,categorical_cols)
])
X_train_transformed = processer.fit_transform(X_train)
X_valid_transformed = processer.transform(X_valid)
#使用梯度提升
model = XGBRegressor(random_state=0,n_estimators=500, learning_rate=0.05, n_jobs=4)


model.fit(X_train_transformed,y_train,early_stopping_rounds=5,eval_set=[(X_valid_transformed, y_valid)],verbose=False)

prediction= model.predict(X_valid_transformed)
print(mean_absolute_error(y_valid,prediction))
#154773.1374148564
```

简单介绍一下上述两段代码用到的参数

+ XGBRegressor

  `xgboost.XGBRegressor` 是 XGBoost（Extreme Gradient Boosting）库中的一个回归模型,基于梯度提升决策树

+ n_estimators

  指定上述建模周期的次数，太大会造成过拟合，太小可能会欠拟合

+ early_stopping_rounds

  提供了一种自动查找 `n_estimators` 的理想值的方法。提前停止会导致模型在验证分数停止提高时停止迭代，即使我们没有在 `n_estimators` 的硬停止。聪明的做法是为 `n_estimators` 设置一个高值，然后使用 `early_stopping_rounds` 来找到停止迭代的最佳时间

  在使用该参数时，还需要留出一些数据用于计算验证分数-这可以通过设置 `eval_set` 参数来完成

+ learning_rate

  模型的学习率，模型预测的并不通过简单的相加来得到，而是乘上一个学习率，并将其添加进去，小的学习率和风多的模型会有更高的准确率，这个值默认为0.1

+ n_jobs

  设置为机器的核心数就好，这和训练速度相关

## Data Leakage

与我们平常所说的信息泄露有很大差别，这是出现在数据集上的

简单来说，泄露会导致模型在训练时看起来很准确，但放到真实的生产环境中会显得无力

主要的两种数据泄露：目标泄露和训练测试污染

### target leakage

简单来说，我们训练的模型预测是经过因果关系，而这个果提前暴露在了训练数据当中。（这里的果并不指预测的值，指强相关性）

比如病人的康复情况，用这种数据来预测病人是否会康复就可能发生目标泄露。

又比如kaggle上的例子，患肺炎与服用药物，服用药物的人一定患肺炎，但是不服用药物的人不一定没有肺炎

> 我觉得kaggle这里的例子更像是在说片面判断的问题，但这两个特征显然是强相关的
>
> 关于特征相关强度可以用**皮尔森相关系数**来看，再进行数据分布对比

[目标泄露](https://zhuanlan.zhihu.com/p/246482947)

### Train-Test Contamination

这个相比target好理解很多，即数据的污染

举个例子，我们在split之前就进行了imputer等操作来处理missing values,这样会导致我们的验证数据和训练数据被同时处理，无论是使用mean还是most_frequent都是错误的选择，model推广新数据的能力显然降低了



